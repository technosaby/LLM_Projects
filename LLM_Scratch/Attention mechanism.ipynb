{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in a short story as text sample into Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The print command prints the total number of characters followed by the first 100\n",
    "characters of this file for illustration purposes. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Our goal is to tokenize this 20,479-character short story into individual words and special\n",
    "characters that we can then turn into embeddings for LLM training  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note that it's common to process millions of articles and hundreds of thousands of\n",
    "books -- many gigabytes of text -- when working with LLMs. However, for educational\n",
    "purposes, it's sufficient to work with smaller text samples like a single book to\n",
    "illustrate the main ideas behind the text processing steps and to make it possible to\n",
    "run it in reasonable time on consumer hardware. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "How can we best split this text to obtain a list of tokens? For this, we go on a small\n",
    "excursion and use Python's regular expression library re for illustration purposes. (Note\n",
    "that you don't have to learn or memorize any regular expression syntax since we will\n",
    "transition to a pre-built tokenizer later in this chapter.) </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Using some simple example text, we can use the re.split command with the following\n",
    "syntax to split a text on whitespace characters:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The result is a list of individual words, whitespaces, and punctuation characters:\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Let's modify the regular expression splits on whitespaces (\\s) and commas, and periods\n",
    "([,.]):</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "We can see that the words and punctuation characters are now separate list entries just as\n",
    "we wanted\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "A small remaining issue is that the list still includes whitespace characters. Optionally, we\n",
    "can remove these redundant characters safely as follows:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "REMOVING WHITESPACES OR NOT\n",
    "\n",
    "\n",
    "When developing a simple tokenizer, whether we should encode whitespaces as\n",
    "separate characters or just remove them depends on our application and its\n",
    "requirements. Removing whitespaces reduces the memory and computing\n",
    "requirements. However, keeping whitespaces can be useful if we train models that\n",
    "are sensitive to the exact structure of the text (for example, Python code, which is\n",
    "sensitive to indentation and spacing). Here, we remove whitespaces for simplicity\n",
    "and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme\n",
    "that includes whitespaces.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The tokenization scheme we devised above works well on the simple sample text. Let's\n",
    "modify it a bit further so that it can also handle other types of punctuation, such as\n",
    "question marks, quotation marks, and the double-dashes we have seen earlier in the first\n",
    "100 characters of Edith Wharton's short story, along with additional special characters: </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "# Strip whitespace from each item and then filter out any empty strings.\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Now that we got a basic tokenizer working, let's apply it to Edith Wharton's entire short\n",
    "story:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In the previous section, we tokenized Edith Wharton's short story and assigned it to a\n",
    "Python variable called preprocessed. Let's now create a list of all unique tokens and sort\n",
    "them alphabetically to determine the vocabulary size:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "After determining that the vocabulary size is 1,130 via the above code, we create the\n",
    "vocabulary and print its first 51 entries for illustration purposes:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As we can see, based on the output above, the dictionary contains individual tokens\n",
    "associated with unique integer labels. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Later in this book, when we want to convert the outputs of an LLM from numbers back into\n",
    "text, we also need a way to turn token IDs into text. \n",
    "\n",
    "For this, we can create an inverse\n",
    "version of the vocabulary that maps token IDs back to corresponding text tokens.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's implement a complete tokenizer class in Python.\n",
    "\n",
    "The class will have an encode method that splits\n",
    "text into tokens and carries out the string-to-integer mapping to produce token IDs via the\n",
    "vocabulary. \n",
    "\n",
    "In addition, we implement a decode method that carries out the reverse\n",
    "integer-to-string mapping to convert the token IDs back into text.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
    "    \n",
    "Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
    "\n",
    "Step 3: Process input text into token IDs\n",
    "\n",
    "Step 4: Convert token IDs back into text\n",
    "\n",
    "Step 5: Replace spaces before the specified punctuation\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class and tokenize a\n",
    "passage from Edith Wharton's short story to try it out in practice:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The code above prints the following token IDs:\n",
    "Next, let's see if we can turn these token IDs back into text using the decode method:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Based on the output above, we can see that the decode method successfully converted the\n",
    "token IDs back into the original text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "So far, so good. We implemented a tokenizer capable of tokenizing and de-tokenizing\n",
    "text based on a snippet from the training set. \n",
    "\n",
    "Let's now apply it to a new text sample that\n",
    "is not contained in the training set:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m      9\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     11\u001b[0m ]\n\u001b[0;32m---> 12\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The problem is that the word \"Hello\" was not used in the The Verdict short story. \n",
    "\n",
    "Hence, it\n",
    "is not contained in the vocabulary. \n",
    "\n",
    "This highlights the need to consider large and diverse\n",
    "training sets to extend the vocabulary when working on LLMs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDING SPECIAL CONTEXT TOKENS\n",
    "\n",
    "In the previous section, we implemented a simple tokenizer and applied it to a passage\n",
    "from the training set. \n",
    "\n",
    "In this section, we will modify this tokenizer to handle unknown\n",
    "words.\n",
    "\n",
    "\n",
    "In particular, we will modify the vocabulary and tokenizer we implemented in the\n",
    "previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and\n",
    "<|endoftext|>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We can modify the tokenizer to use an <|unk|> token if it\n",
    "encounters a word that is not part of the vocabulary. \n",
    "\n",
    "Furthermore, we add a token between\n",
    "unrelated texts. \n",
    "\n",
    "For example, when training GPT-like LLMs on multiple independent\n",
    "documents or books, it is common to insert a token before each document or book that\n",
    "follows a previous text source\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's now modify the vocabulary to include these two special tokens, <unk> and\n",
    "<|endoftext|>, by adding these to the list of all unique words that we created in the\n",
    "previous section:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Based on the output of the print statement above, the new vocabulary size is 1132 (the\n",
    "vocabulary size in the previous section was 1130).\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "As an additional quick check, let's print the last 5 entries of the updated vocabulary:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "A simple text tokenizer that handles unknown words</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Replace unknown words by <|unk|> tokens\n",
    "    \n",
    "Step 2: Replace spaces before the specified punctuations\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Based on comparing the de-tokenized text above with the original input text, we know that\n",
    "the training dataset, Edith Wharton's short story The Verdict, did not contain the words\n",
    "\"Hello\" and \"palace.\"\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "So far, we have discussed tokenization as an essential step in processing text as input to\n",
    "LLMs. Depending on the LLM, some researchers also consider additional special tokens such\n",
    "as the following:\n",
    "\n",
    "[BOS] (beginning of sequence): This token marks the start of a text. It\n",
    "signifies to the LLM where a piece of content begins.\n",
    "\n",
    "[EOS] (end of sequence): This token is positioned at the end of a text,\n",
    "and is especially useful when concatenating multiple unrelated texts,\n",
    "similar to <|endoftext|>. For instance, when combining two different\n",
    "Wikipedia articles or books, the [EOS] token indicates where one article\n",
    "ends and the next one begins.\n",
    "\n",
    "[PAD] (padding): When training LLMs with batch sizes larger than one,\n",
    "the batch might contain texts of varying lengths. To ensure all texts have\n",
    "the same length, the shorter texts are extended or \"padded\" using the\n",
    "[PAD] token, up to the length of the longest text in the batch.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note that the tokenizer used for GPT models does not need any of these tokens mentioned\n",
    "above but only uses an <|endoftext|> token for simplicity\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "the tokenizer used for GPT models also doesn't use an <|unk|> token for outof-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer, which breaks\n",
    "down words into subword units\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BYTE PAIR ENCODING (BPE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We implemented a simple tokenization scheme in the previous sections for illustration\n",
    "purposes. \n",
    "\n",
    "This section covers a more sophisticated tokenization scheme based on a concept\n",
    "called byte pair encoding (BPE). \n",
    "\n",
    "The BPE tokenizer covered in this section was used to train\n",
    "LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Since implementing BPE can be relatively complicated, we will use an existing Python\n",
    "open-source library called tiktoken (https://github.com/openai/tiktoken). \n",
    "\n",
    "This library implements\n",
    "the BPE algorithm very efficiently based on source code in Rust.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Users/ghs6kor/miniconda3/envs/llm_test/lib/python3.13/site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/ghs6kor/miniconda3/envs/llm_test/lib/python3.13/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/ghs6kor/miniconda3/envs/llm_test/lib/python3.13/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ghs6kor/miniconda3/envs/llm_test/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ghs6kor/miniconda3/envs/llm_test/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ghs6kor/miniconda3/envs/llm_test/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ghs6kor/miniconda3/envs/llm_test/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Once installed, we can instantiate the BPE tokenizer from tiktoken as follows:</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The usage of this tokenizer is similar to SimpleTokenizerV2 we implemented previously via\n",
    "an encode method:</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The code above prints the following token IDs:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "We can then convert the token IDs back into text using the decode method, similar to our\n",
    "SimpleTokenizerV2 earlier:</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We can make two noteworthy observations based on the token IDs and decoded text\n",
    "above. \n",
    "\n",
    "First, the <|endoftext|> token is assigned a relatively large token ID, namely,\n",
    "50256. \n",
    "\n",
    "In fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3,\n",
    "and the original model used in ChatGPT, has a total vocabulary size of 50,257, with\n",
    "<|endoftext|> being assigned the largest token ID.\n",
    "    \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Second, the BPE tokenizer above encodes and decodes unknown words, such as\n",
    "\"someunknownPlace\" correctly. \n",
    "\n",
    "The BPE tokenizer can handle any unknown word. How does\n",
    "it achieve this without using <|unk|> tokens?\n",
    "    \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The algorithm underlying BPE breaks down words that aren't in its predefined vocabulary\n",
    "into smaller subword units or even individual characters.\n",
    "\n",
    "The enables it to handle out-ofvocabulary words. \n",
    "\n",
    "So, thanks to the BPE algorithm, if the tokenizer encounters an\n",
    "unfamiliar word during tokenization, it can represent it as a sequence of subword tokens or\n",
    "characters\n",
    "    \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us take another simple example to illustrate how the BPE tokenizer deals with unknown tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size for GPT2 is: 50257\n",
      "The vocabulary size for GPT3 is: 50281\n",
      "The vocabulary size for GPT4 is: 100277\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Initialize the encodings for GPT-2, GPT-3, and GPT-4\n",
    "encodings = {\n",
    "    \"gpt2\": tiktoken.get_encoding(\"gpt2\"),\n",
    "    \"gpt3\": tiktoken.get_encoding(\"p50k_base\"),  # Commonly associated with GPT-3 models\n",
    "    \"gpt4\": tiktoken.get_encoding(\"cl100k_base\")  # Used for GPT-4 and later versions\n",
    "}\n",
    "\n",
    "# Get the vocabulary size for each encoding\n",
    "vocab_sizes = {model: encoding.n_vocab for model, encoding in encodings.items()}\n",
    "\n",
    "# Print the vocabulary sizes\n",
    "for model, size in vocab_sizes.items():\n",
    "    print(f\"The vocabulary size for {model.upper()} is: {size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING INPUT-TARGET PAIRS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "In this section we implement a data loader that fetches the input-target pairs using a sliding window approach.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "To get started, we will first tokenize the whole The Verdict short story we worked with\n",
    "earlier using the BPE tokenizer introduced in the previous section:</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Executing the code above will return 5145, the total number of tokens in the training set,\n",
    "after applying the BPE tokenizer.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Next, we remove the first 50 tokens from the dataset for demonstration purposes as it\n",
    "results in a slightly more interesting text passage in the next steps:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "One of the easiest and most intuitive ways to create the input-target pairs for the nextword prediction task is to create two variables, x and y, where x contains the input tokens\n",
    "and y contains the targets, which are the inputs shifted by 1:</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The context size determines how many tokens are included in the input\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 #length of the input\n",
    "#The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens) \n",
    "#to predict the next word in the sequence. \n",
    "#The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 tokens [2, 3, 4, 5]\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Processing the inputs along with the targets, which are the inputs shifted by one position,\n",
    "we can then create the next-word prediction tasks as\n",
    "follows:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Everything left of the arrow (---->) refers to the input an LLM would receive, and the token\n",
    "ID on the right side of the arrow represents the target token ID that the LLM is supposed to\n",
    "predict.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "For illustration purposes, let's repeat the previous code but convert the token IDs into\n",
    "text:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We've now created the input-target pairs that we can turn into use for the LLM training in\n",
    "upcoming chapters.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "There's only one more task before we can turn the tokens into embeddings:implementing an efficient data loader that\n",
    "iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which\n",
    "can be thought of as multidimensional arrays.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In particular, we are interested in returning two tensors: an input tensor containing the\n",
    "text that the LLM sees and a target tensor that includes the targets for the LLM to predict,\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTING A DATA LOADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "For the efficient data loader implementation, we will use PyTorch's built-in Dataset and\n",
    "DataLoader classes.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Tokenize the entire text\n",
    "    \n",
    "Step 2: Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "\n",
    "Step 3: Return the total number of rows in the dataset\n",
    "\n",
    "Step 4: Return a single row from the dataset\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The GPTDatasetV1 class in listing 2.5 is based on the PyTorch Dataset class.\n",
    "\n",
    "It defines how individual rows are fetched from the dataset. \n",
    "\n",
    "Each row consists of a number of\n",
    "token IDs (based on a max_length) assigned to an input_chunk tensor. \n",
    "\n",
    "The target_chunk\n",
    "tensor contains the corresponding targets. \n",
    "\n",
    "I recommend reading on to see how the data\n",
    "returned from this dataset looks like when we combine the dataset with a PyTorch\n",
    "DataLoader -- this will bring additional intuition and clarity.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The following code will use the GPTDatasetV1 to load the inputs in batches via a PyTorch\n",
    "DataLoader:</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Initialize the tokenizer\n",
    "\n",
    "Step 2: Create dataset\n",
    "\n",
    "Step 3: drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes\n",
    "during training\n",
    "\n",
    "Step 4: The number of CPU processes to use for preprocessing\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4, \n",
    "\n",
    "This will develop an intuition of how the GPTDatasetV1 class and the\n",
    "create_dataloader_v1 function work together: </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Convert dataloader into a Python iterator to fetch the next entry via Python's built-in next() function\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.0\n",
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n",
      "[tensor([[2885, 1464, 1807, 3619]]), tensor([[1464, 1807, 3619,  402]])]\n",
      "[tensor([[1464, 1807, 3619,  402]]), tensor([[1807, 3619,  402,  271]])]\n",
      "[tensor([[1807, 3619,  402,  271]]), tensor([[ 3619,   402,   271, 10899]])]\n",
      "[tensor([[ 3619,   402,   271, 10899]]), tensor([[  402,   271, 10899,  2138]])]\n",
      "[tensor([[  402,   271, 10899,  2138]]), tensor([[  271, 10899,  2138,   257]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The first_batch variable contains two tensors: the first tensor stores the input token IDs,\n",
    "and the second tensor stores the target token IDs. \n",
    "\n",
    "Since the max_length is set to 4, each of the two tensors contains 4 token IDs. \n",
    "\n",
    "Note that an input size of 4 is relatively small and only chosen for illustration purposes. It is common to train LLMs with input sizes of at least\n",
    "256.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "To illustrate the meaning of stride=1, let's fetch another batch from this dataset: </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  271, 10899,  2138,   257]]), tensor([[10899,  2138,   257,  7026]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "If we compare the first with the second batch, we can see that the second batch's token\n",
    "IDs are shifted by one position compared to the first batch. \n",
    "\n",
    "For example, the second ID in\n",
    "the first batch's input is 367, which is the first ID of the second batch's input. \n",
    "\n",
    "The stride\n",
    "setting dictates the number of positions the inputs shift across batches, emulating a sliding\n",
    "window approach\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Batch sizes of 1, such as we have sampled from the data loader so far, are useful for\n",
    "illustration purposes. \n",
    "                                                                                 \n",
    "If you have previous experience with deep learning, you may know\n",
    "that small batch sizes require less memory during training but lead to more noisy model\n",
    "updates.\n",
    "\n",
    "Just like in regular deep learning, the batch size is a trade-off and hyperparameter\n",
    "to experiment with when training LLMs.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Before we move on to the two final sections of this chapter that are focused on creating\n",
    "the embedding vectors from the token IDs, let's have a brief look at how we can use the\n",
    "data loader to sample with a batch size greater than 1: </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Note that we increase the stride to 4. This is to utilize the data set fully (we don't skip a\n",
    "single word) but also avoid any overlap between the batches, since more overlap could lead\n",
    "to increased overfitting.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING TOKEN EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Let's illustrate how the token ID to embedding vector conversion works with a hands-on\n",
    "example. Suppose we have the following four input tokens with IDs 2, 3, 5, and 1:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "For the sake of simplicity and illustration purposes, suppose we have a small vocabulary of\n",
    "only 6 words (instead of the 50,257 words in the BPE tokenizer vocabulary), and we want\n",
    "to create embeddings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Using the vocab_size and output_dim, we can instantiate an embedding layer in PyTorch,\n",
    "setting the random seed to 123 for reproducibility purposes:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The print statement in the code prints the embedding layer's underlying\n",
    "weight matrix:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "We can see that the weight matrix of the embedding layer contains small, random values.\n",
    "These values are optimized during LLM training as part of the LLM optimization itself, as we\n",
    "will see in upcoming chapters. Moreover, we can see that the weight matrix has six rows\n",
    "and three columns. There is one row for each of the six possible tokens in the vocabulary.\n",
    "And there is one column for each of the three embedding dimensions.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "After we instantiated the embedding layer, let's now apply it to a token ID to obtain the\n",
    "embedding vector:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "If we compare the embedding vector for token ID 3 to the previous embedding matrix, we\n",
    "see that it is identical to the 4th row (Python starts with a zero index, so it's the row\n",
    "corresponding to index 3). In other words, the embedding layer is essentially a look-up\n",
    "operation that retrieves rows from the embedding layer's weight matrix via a token ID.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Previously, we have seen how to convert a single token ID into a three-dimensional\n",
    "embedding vector. Let's now apply that to all four input IDs we defined earlier\n",
    "(torch.tensor([2, 3, 5, 1])):\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Each row in this output matrix is obtained via a lookup operation from the embedding\n",
    "weight matrix\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSITIONAL EMBEDDINGS (ENCODING WORD POSITIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Previously, we focused on very small embedding sizes in this chapter for illustration\n",
    "purposes. \n",
    "\n",
    "We now consider more realistic and useful embedding sizes and encode the input\n",
    "tokens into a 256-dimensional vector representation. \n",
    "\n",
    "This is smaller than what the original\n",
    "GPT-3 model used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable\n",
    "for experimentation. \n",
    "\n",
    "Furthermore, we assume that the token IDs were created by the BPE\n",
    "tokenizer that we implemented earlier, which has a vocabulary size of 50,257:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Using the token_embedding_layer above, if we sample data from the data loader, we\n",
    "embed each token in each batch into a 256-dimensional vector. If we have a batch size of 8\n",
    "with four tokens each, the result will be an 8 x 4 x 256 tensor.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's instantiate the data loader ( Data sampling with a sliding window),\n",
    "first:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As we can see, the token ID tensor is 8x4-dimensional, meaning that the data batch\n",
    "consists of 8 text samples with 4 tokens each.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's now use the embedding layer to embed these token IDs into 256-dimensional\n",
    "vectors:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As we can tell based on the 8x4x256-dimensional tensor output, each token ID is now\n",
    "embedded as a 256-dimensional vector.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "For a GPT model's absolute embedding approach, we just need to create another\n",
    "embedding layer that has the same dimension as the token_embedding_layer:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As shown in the preceding code example, the input to the pos_embeddings is usually a\n",
    "placeholder vector torch.arange(context_length), which contains a sequence of\n",
    "numbers 0, 1, ..., up to the maximum input length  1. \n",
    "\n",
    "The context_length is a variable\n",
    "that represents the supported input size of the LLM. \n",
    "\n",
    "Here, we choose it similar to the\n",
    "maximum length of the input text. \n",
    "\n",
    "In practice, input text can be longer than the supported\n",
    "context length, in which case we have to truncate the text.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As we can see, the positional embedding tensor consists of four 256-dimensional vectors.\n",
    "We can now add these directly to the token embeddings, where PyTorch will add the 4x256-\n",
    "dimensional pos_embeddings tensor to each 4x256-dimensional token embedding tensor in\n",
    "each of the 8 batches:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The input_embeddings we created are the embedded input\n",
    "examples that can now be processed by the main LLM modules\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTING A SIMPLIFIED ATTENTION MECHANISM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Consider the following input sentence, which has already been embedded into 3-\n",
    "dimensional vectors. \n",
    "\n",
    "We choose a small embedding dimension for\n",
    "illustration purposes to ensure it fits on the page without line breaks:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmpl_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmplot3d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Axes3D\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Corresponding words\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Corresponding words\n",
    "words = ['Your', 'journey', 'starts', 'with', 'one', 'step']\n",
    "\n",
    "# Extract x, y, z coordinates\n",
    "x_coords = inputs[:, 0].numpy()\n",
    "y_coords = inputs[:, 1].numpy()\n",
    "z_coords = inputs[:, 2].numpy()\n",
    "\n",
    "# Create 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each point and annotate with corresponding word\n",
    "for x, y, z, word in zip(x_coords, y_coords, z_coords, words):\n",
    "    ax.scatter(x, y, z)\n",
    "    ax.text(x, y, z, word, fontsize=10)\n",
    "\n",
    "# Set labels for axes\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "plt.title('3D Plot of Word Embeddings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create 3D plot with vectors from origin to each point, using different colors\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Define a list of colors for the vectors\n",
    "colors = ['r', 'g', 'b', 'c', 'm', 'y']\n",
    "\n",
    "# Plot each vector with a different color and annotate with the corresponding word\n",
    "for (x, y, z, word, color) in zip(x_coords, y_coords, z_coords, words, colors):\n",
    "    # Draw vector from origin to the point (x, y, z) with specified color and smaller arrow length ratio\n",
    "    ax.quiver(0, 0, 0, x, y, z, color=color, arrow_length_ratio=0.05)\n",
    "    ax.text(x, y, z, word, fontsize=10, color=color)\n",
    "\n",
    "# Set labels for axes\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "# Set plot limits to keep arrows within the plot boundaries\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_zlim([0, 1])\n",
    "\n",
    "plt.title('3D Plot of Word Embeddings with Colored Vectors')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Each row represents a word, and each column represents an embedding dimension\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The second input token serves as the query    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # 2nd input token is the query\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the next step, we normalize each of the attention scores that\n",
    "we computed previously.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The main goal behind the normalization  is to obtain attention weights\n",
    "that sum up to 1. \n",
    "\n",
    "This normalization is a convention that is useful for interpretation and for\n",
    "maintaining training stability in an LLM. \n",
    "\n",
    "Here's a straightforward method for achieving this\n",
    "normalization step:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "In practice, it's more common and advisable to use the softmax function for normalization.\n",
    "\n",
    "This approach is better at managing extreme values and offers more favorable gradient\n",
    "properties during training. \n",
    "\n",
    "Below is a basic implementation of the softmax function for\n",
    "normalizing the attention scores: \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As the output shows, the softmax function also meets the objective and normalizes the\n",
    "attention weights such that they sum to 1:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In addition, the softmax function ensures that the attention weights are always positive.\n",
    "This makes the output interpretable as probabilities or relative importance, where higher\n",
    "weights indicate greater importance.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note that this naive softmax implementation (softmax_naive) may encounter numerical\n",
    "instability problems, such as overflow and underflow, when dealing with large or small input\n",
    "values. \n",
    "\n",
    "Therefore, in practice, it's advisable to use the PyTorch implementation of softmax,\n",
    "which has been extensively optimized for performance:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "In this case, we can see that it yields the same results as our previous softmax_naive\n",
    "function:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The context vector z(2)is calculated as a weighted sum of all input\n",
    "vectors. \n",
    "\n",
    "This involves multiplying each input vector by its corresponding attention weight:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # 2nd input token is the query\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmpl_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmplot3d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Axes3D\n\u001b[1;32m      4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m      5\u001b[0m   [[\u001b[38;5;241m0.43\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0.89\u001b[39m], \u001b[38;5;66;03m# Your     (x^1)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m    [\u001b[38;5;241m0.55\u001b[39m, \u001b[38;5;241m0.87\u001b[39m, \u001b[38;5;241m0.66\u001b[39m], \u001b[38;5;66;03m# journey  (x^2)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m    [\u001b[38;5;241m0.4419\u001b[39m, \u001b[38;5;241m0.6515\u001b[39m, \u001b[38;5;241m0.5683\u001b[39m]]\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55], # step     (x^6)\n",
    "   [0.4419, 0.6515, 0.5683]]\n",
    ")\n",
    "\n",
    "# Corresponding words\n",
    "words = ['Your', 'journey', 'starts', 'with', 'one', 'step', 'journey-context']\n",
    "\n",
    "# Extract x, y, z coordinates\n",
    "x_coords = inputs[:, 0].numpy()\n",
    "y_coords = inputs[:, 1].numpy()\n",
    "z_coords = inputs[:, 2].numpy()\n",
    "\n",
    "# Create 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each point and annotate with corresponding word\n",
    "for x, y, z, word in zip(x_coords, y_coords, z_coords, words):\n",
    "    ax.scatter(x, y, z)\n",
    "    ax.text(x, y, z, word, fontsize=10)\n",
    "\n",
    "# Set labels for axes\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "plt.title('3D Plot of Word Embeddings')\n",
    "plt.show()\n",
    "\n",
    "# Create 3D plot with vectors from origin to each point, using different colors\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Define a list of colors for the vectors\n",
    "colors = ['r', 'g', 'b', 'c', 'm', 'y', 'r']\n",
    "\n",
    "# Plot each vector with a different color and annotate with the corresponding word\n",
    "for (x, y, z, word, color) in zip(x_coords, y_coords, z_coords, words, colors):\n",
    "    # Draw vector from origin to the point (x, y, z) with specified color and smaller arrow length ratio\n",
    "    ax.quiver(0, 0, 0, x, y, z, color=color, arrow_length_ratio=0.05)\n",
    "    ax.text(x, y, z, word, fontsize=10, color=color)\n",
    "\n",
    "# Set labels for axes\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "# Set plot limits to keep arrows within the plot boundaries\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_zlim([0, 1])\n",
    "\n",
    "plt.title('3D Plot of Word Embeddings with Colored Vectors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Now, we can extend this computation to\n",
    "calculate attention weights and context vectors for all inputs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "First, we add an additional for-loop to compute the\n",
    "dot products for all pairs of inputs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Each element in the preceding tensor represents an attention score between each pair of\n",
    "inputs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "When computing the preceding attention score tensor, we used for-loops in Python.\n",
    "                                                            \n",
    "However, for-loops are generally slow, and we can achieve the same results using matrix\n",
    "multiplication:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We now normalize each row so that the values in\n",
    "each row sum to 1:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In the context of using PyTorch, the dim parameter in functions like torch.softmax specifies\n",
    "the dimension of the input tensor along which the function will be computed. \n",
    "\n",
    "By setting\n",
    "dim=-1, we are instructing the softmax function to apply the normalization along the last\n",
    "dimension of the attn_scores tensor. \n",
    "\n",
    "If attn_scores is a 2D tensor (for example, with a\n",
    "shape of [rows, columns]), dim=-1 will normalize across the columns so that the values in\n",
    "each row (summing over the column dimension) sum up to 1.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's briefly verify that\n",
    "the rows indeed all sum to 1:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the third and last step, we now use these attention weights to compute all context\n",
    "vectors via matrix multiplication:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We can double-check that the code is correct by comparing the 2nd row with the context\n",
    "vector z(2) calculated previously\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"Previous 2nd context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Based on the result, we can see that the previously calculated context_vec_2 matches the\n",
    "second row in the previous tensor exactly\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "This concludes the code walkthrough of a simple self-attention mechanism.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTING SELF ATTENTION WITH TRAINABLE WEIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's begin by defining a few variables:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "#A The second input element\n",
    "\n",
    "#B The input embedding size, d=3\n",
    "\n",
    "\n",
    "#C The output embedding size, d_out=2\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] #A\n",
    "d_in = inputs.shape[1] #B\n",
    "d_out = 2 #C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Note that in GPT-like models, the input and output dimensions are usually the same. \n",
    "\n",
    "But for illustration purposes, to better follow the computation, we choose different input (d_in=3)\n",
    "and output (d_out=2) dimensions here.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, we initialize the three weight matrices Wq, Wk and Wv\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Note that we are setting requires_grad=False to reduce clutter in the outputs for\n",
    "illustration purposes. \n",
    "\n",
    "If we were to use the weight matrices for model training, we\n",
    "would set requires_grad=True to update these matrices during model training.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, we compute the query, key, and value vectors as shown earlier\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As we can see based on the output for the query, this results in a 2-dimensional vector. \n",
    "\n",
    "This is because: we set the number of columns of the corresponding weight matrix, via d_out, to 2:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Even though our temporary goal is to only compute the one context vector z(2),  we still\n",
    "require the key and value vectors for all input elements. \n",
    "\n",
    "This is because they are involved in computing the attention weights with respect to the query q(2)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We can obtain all keys and values via matrix multiplication:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As we can tell from the outputs, we successfully projected the 6 input tokens from a 3D\n",
    "onto a 2D embedding space:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "First, let's compute the attention score 22</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1] #A\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Again, we can generalize this computation to all attention scores via matrix multiplication:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "We compute the attention weights by scaling the\n",
    "attention scores and using the softmax function we used earlier. \n",
    "\n",
    "The difference to earlier is\n",
    "that we now scale the attention scores by dividing them by the square root of the\n",
    "embedding dimension of the keys. \n",
    "\n",
    "Note that taking the square root is mathematically the\n",
    "same as exponentiating by 0.5:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "We now compute the context vector as a weighted sum over the value\n",
    "vectors. \n",
    "\n",
    "Here, the attention weights serve as a weighting factor that weighs the respective\n",
    "importance of each value vector. \n",
    "\n",
    "We can use matrix multiplication to\n",
    "obtain the output in one step:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "So far, we only computed a single context vector, z(2). \n",
    "\n",
    "In the next section, we will generalize the code to compute all context vectors in the input sequence, z(1)to z (T)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTING A COMPACT SELF ATTENTION PYTHON CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "In the previous sections, we have gone through a lot of steps to compute the self-attention\n",
    "outputs. \n",
    "\n",
    "This was mainly done for illustration purposes so we could go through one step at\n",
    "a time. \n",
    "\n",
    "In practice, with the LLM implementation in the next chapter in mind, it is helpful to\n",
    "organize this code into a Python class as follows:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In this PyTorch code, SelfAttention_v1 is a class derived from nn.Module, which is a\n",
    "fundamental building block of PyTorch models, which provides necessary functionalities for\n",
    "model layer creation and management.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The __init__ method initializes trainable weight matrices (W_query, W_key, and\n",
    "W_value) for queries, keys, and values, each transforming the input dimension d_in to an\n",
    "output dimension d_out.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "During the forward pass, using the forward method, we compute the attention scores\n",
    "(attn_scores) by multiplying queries and keys, normalizing these scores using softmax.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Finally, we create a context vector by weighting the values with these normalized attention\n",
    "scores.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Since inputs contains six embedding vectors, we get a matrix storing the six\n",
    "context vectors, as shown in the above result. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As a quick check, notice how the second row ([0.3061, 0.8210]) matches the contents of\n",
    "context_vec_2 in the previous section.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We can improve the SelfAttention_v1 implementation further by utilizing PyTorch's\n",
    "nn.Linear layers, which effectively perform matrix multiplication when the bias units are\n",
    "disabled. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Additionally, a significant advantage of using nn.Linear instead of manually\n",
    "implementing nn.Parameter(torch.rand(...)) is that nn.Linear has an optimized weight\n",
    "initialization scheme, contributing to more stable and effective model training.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "You can use the SelfAttention_v2 similar to SelfAttention_v1:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because they\n",
    "use different initial weights for the weight matrices since nn.Linear uses a more\n",
    "sophisticated weight initialization scheme.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIDING FUTURE WORDS WITH CAUSAL ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's work with the attention scores and weights from the previous section to code the causal attention mechanism.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the first step illustrated in Figure 3.20, we compute the attention weights using the\n",
    "softmax function as we have done in previous sections:    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Reuse the query and key weight matrices of the SelfAttention_v2 object from the previous section for\n",
    "convenience\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs) #A\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We can now use PyTorch's tril function to create a mask\n",
    "where the values above the diagonal are zero:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Now, we can multiply this mask with the attention weights to zero out the values above the\n",
    "diagonal:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see, the elements above the diagonal are successfully zeroed out\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The third step is to renormalize the attention weights to sum up to 1 again in\n",
    "each row. \n",
    "\n",
    "We can achieve this by dividing each element in each row by the sum in each\n",
    "row:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The result is an attention weight matrix where the attention weights above the diagonal are\n",
    "zeroed out and where the rows sum to 1.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "While we could be technically done with implementing causal attention at this point, we can\n",
    "take advantage of a mathematical property of the softmax function. \n",
    "\n",
    "We can implement the computation of the masked attention weights more efficiently in fewer steps.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The softmax function converts its inputs into a probability distribution. \n",
    "\n",
    "When negative\n",
    "infinity values (-) are present in a row, the softmax function treats them as zero\n",
    "probability. \n",
    "\n",
    "(Mathematically, this is because e\n",
    "- approaches 0.)\n",
    "\n",
    "\n",
    "We can implement this more efficient masking \"trick\" by creating a mask with 1's above\n",
    "the diagonal and then replacing these 1's with negative infinity (-inf) values:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Now, all we need to do is apply the softmax function to these masked results, and we are\n",
    "done.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see based on the output, the values in each row sum to 1, and no further\n",
    "normalization is necessary.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Masking in Transformers sets scores for future tokens to a large negative value, making their influence in the softmax calculation effectively zero. \n",
    "\n",
    "The softmax function then recalculates attention weights only among the unmasked tokens. \n",
    "\n",
    "This process ensures no information leakage from masked tokens, focusing the model solely on the intended data.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We could now use the modified attention weights to compute the context vectors via\n",
    "context_vec = attn_weights @ values.\n",
    "\n",
    "However, in the next section,\n",
    "we first cover another minor tweak to the causal attention mechanism that is useful for\n",
    "reducing overfitting when training LLMs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MASKING ADDITIONAL ATTENTION WEIGHTS WITH DROPOUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the following code example, we use a dropout rate of 50%, which means masking out\n",
    "half of the attention weights.\n",
    "\n",
    "When we train the GPT model in later chapters, we will use a\n",
    "lower dropout rate, such as 0.1 or 0.2.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the following code, we apply PyTorch's dropout implementation first to a 66 tensor\n",
    "consisting of ones for illustration purposes:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) #A\n",
    "example = torch.ones(6, 6) #B\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "When applying dropout to an attention weight matrix with a rate of 50%, half of the\n",
    "elements in the matrix are randomly set to zero. \n",
    "\n",
    "To compensate for the reduction in active\n",
    "elements, the values of the remaining elements in the matrix are scaled up by a factor of\n",
    "1/0.5 =2. \n",
    "\n",
    "This scaling is crucial to maintain the overall balance of the attention weights,\n",
    "ensuring that the average influence of the attention mechanism remains consistent during\n",
    "both the training and inference phases.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Now, let's apply dropout to the attention weight matrix itself:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see above, the resulting attention weight matrix now has additional elements zeroed out and the\n",
    "remaining ones rescaled.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Having gained an understanding of causal attention and dropout masking, we will\n",
    "develop a concise Python class in the following section. \n",
    "\n",
    "This class is designed to facilitate\n",
    "the efficient application of these two techniques.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTING A COMPACT CAUSAL ATTENTION CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In this section, we will now incorporate the causal attention and dropout modifications into\n",
    "the SelfAttention Python class we developed in section 3.4. \n",
    "\n",
    "This class will then serve as a\n",
    "template for developing multi-head attention in the upcoming section.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Before we begin, one more thing is to ensure that the code can handle batches\n",
    "consisting of more than one input. \n",
    "\n",
    "This will ensure that the CausalAttention class supports the batch\n",
    "outputs produced by the data loader we implemented earlier.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "For simplicity, to simulate such batch inputs, we duplicate the input text example:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    " 2 inputs with 6 tokens each, and each token has embedding dimension 3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "This results in a 3D tensor consisting of 2 input texts with 6 tokens each, where each token\n",
    "is a 3-dimensional embedding vector.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The following CausalAttention class is similar to the SelfAttention class we\n",
    "implemented earlier, except that we now added the dropout and causal mask components\n",
    "as highlighted in the following code.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Step 1: Compared to the previous SelfAttention_v1 class, we added a dropout layer.\n",
    "    \n",
    "Step 2: The register_buffer call is also a new addition (more information is provided in the following text).\n",
    "\n",
    "Step 3:  We transpose dimensions 1 and 2, keeping the batch dimension at the first position (0).\n",
    "\n",
    "Step 4: In PyTorch, operations with a trailing underscore are performed in-place, avoiding unnecessary memory\n",
    "copies\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The use of register_buffer in\n",
    "PyTorch is not strictly necessary for all use cases but offers several advantages here. \n",
    "\n",
    "For\n",
    "instance, when we use the CausalAttention class in our LLM, buffers are automatically\n",
    "moved to the appropriate device (CPU or GPU) along with our model, which will be relevant\n",
    "when training the LLM in future chapters. \n",
    "\n",
    "This means we don't need to manually ensure\n",
    "these tensors are on the same device as your model parameters, avoiding device mismatch\n",
    "errors.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We can use the CausalAttention class as follows, similar to SelfAttention previously:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see, the resulting context vector is a 3D tensor where each token is now represented by a 2D\n",
    "embedding:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In the next section, we will expand on this concept\n",
    "and implement a multi-head attention module, that implements several of such causal\n",
    "attention mechanisms in parallel.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTENDING SINGLE HEAD ATTENTION TO MULTI-HEAD ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "In practical terms, implementing multi-head attention involves creating multiple instances\n",
    "of the self-attention mechanism, each with\n",
    "its own weights, and then combining their outputs\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "In code, we can achieve this by implementing a simple MultiHeadAttentionWrapper\n",
    "class that stacks multiple instances of our previously implemented CausalAttention\n",
    "module:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "For example, if we use this MultiHeadAttentionWrapper class with two attention heads (via\n",
    "num_heads=2) and CausalAttention output dimension d_out=2, this results in a 4-\n",
    "dimensional context vectors (d_out*num_heads=4)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "To illustrate further with a concrete example, we can use the\n",
    "MultiHeadAttentionWrapper class similar to the CausalAttention class before:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The first dimension of the resulting context_vecs tensor is 2 since we have two input texts\n",
    "(the input texts are duplicated, which is why the context vectors are exactly the same for\n",
    "those). \n",
    "\n",
    "The second dimension refers to the 6 tokens in each input. The third dimension\n",
    "refers to the 4-dimensional embedding of each token.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "In this section, we implemented a MultiHeadAttentionWrapper that combined multiple\n",
    "single-head attention modules. \n",
    "\n",
    "However, note that these are processed sequentially via\n",
    "[head(x) for head in self.heads] in the forward method. \n",
    "\n",
    "We can improve this\n",
    "implementation by processing the heads in parallel. \n",
    "\n",
    "One way to achieve this is by\n",
    "computing the outputs for all attention heads simultaneously via matrix multiplication, as\n",
    "we will explore in the next section.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTING MULTI-HEAD ATTENTION WITH WEIGHT SPLITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "Instead of maintaining two separate classes, MultiHeadAttentionWrapper and\n",
    "CausalAttention, we can combine both of these concepts into a single\n",
    "MultiHeadAttention class. \n",
    "\n",
    "Also, in addition to just merging the\n",
    "MultiHeadAttentionWrapper with the CausalAttention code, we will make some other\n",
    "modifications to implement multi-head attention more efficiently.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "In the MultiHeadAttentionWrapper, multiple heads are implemented by creating a list\n",
    "of CausalAttention objects (self.heads), each representing a separate attention head.\n",
    "\n",
    "\n",
    "The CausalAttention class independently performs the attention mechanism, and the\n",
    "results from each head are concatenated.\n",
    "\n",
    "In contrast, the following MultiHeadAttention\n",
    "class integrates the multi-head functionality within a single class. \n",
    "\n",
    "\n",
    "It splits the input into\n",
    "multiple heads by reshaping the projected query, key, and value tensors and then combines\n",
    "the results from these heads after computing attention.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's take a look at the MultiHeadAttention class before we discuss it further:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Step 1: Reduce the projection dim to match desired output dim\n",
    "\n",
    "Step 2: Use a Linear layer to combine head outputs\n",
    "\n",
    "Step 3: Tensor shape: (b, num_tokens, d_out)\n",
    "\n",
    "Step 4: We implicitly split the matrix by adding a `num_heads` dimension. Then we unroll last dim: (b,\n",
    "num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "\n",
    "Step 5: Transpose from shape (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)\n",
    "\n",
    "Step 6: Compute dot product for each head\n",
    "\n",
    "Step 7: Mask truncated to the number of tokens\n",
    "\n",
    "Step 8: Use the mask to fill attention scores\n",
    "\n",
    "Step 9: Tensor shape: (b, num_tokens, n_heads, head_dim)\n",
    "\n",
    "Step 10: Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "\n",
    "Step 11: Add an optional linear projection\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Even though the reshaping (.view) and transposing (.transpose) of tensors inside the\n",
    "MultiHeadAttention class looks very complicated, mathematically, the\n",
    "MultiHeadAttention class implements the same concept as the\n",
    "MultiHeadAttentionWrapper earlier.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "On a big-picture level, in the previous MultiHeadAttentionWrapper, we stacked\n",
    "multiple single-head attention layers that we combined into a multi-head attention layer.\n",
    "\n",
    "\n",
    "The MultiHeadAttention class takes an integrated approach. \n",
    "\n",
    "It starts with a multi-head\n",
    "layer and then internally splits this layer into individual attention heads\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DETAILED EXPLANATION OF THE MULTI-HEAD ATTENTION CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The splitting of the query, key, and value tensors, is achieved\n",
    "through tensor reshaping and transposing operations using PyTorch's .view and\n",
    ".transpose methods. \n",
    "\n",
    "The input is first transformed (via linear layers for queries, keys, and\n",
    "values) and then reshaped to represent multiple heads.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The key operation is to split the d_out dimension into num_heads and head_dim, where\n",
    "head_dim = d_out / num_heads. \n",
    "\n",
    "This splitting is then achieved using the .view method: a\n",
    "tensor of dimensions (b, num_tokens, d_out) is reshaped to dimension (b, num_tokens,\n",
    "num_heads, head_dim).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The tensors are then transposed to bring the num_heads dimension before the\n",
    "num_tokens dimension, resulting in a shape of (b, num_heads, num_tokens, head_dim).\n",
    "\n",
    "This transposition is crucial for correctly aligning the queries, keys, and values across the\n",
    "different heads and performing batched matrix multiplications efficiently.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "To illustrate this batched matrix multiplication, suppose we have the following example\n",
    "tensor:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573], #A\n",
    "[0.8993, 0.0390, 0.9268, 0.7388],\n",
    "[0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "[[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "[0.4066, 0.2318, 0.4545, 0.9737],\n",
    "[0.4606, 0.5159, 0.4220, 0.5786]]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The shape of this tensor is (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Now, we perform a batched matrix multiplication between the tensor itself and a view of\n",
    "the tensor where we transposed the last two dimensions, num_tokens and head_dim:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "source": [
    "print(a @ a.transpose(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "In this case, the matrix multiplication implementation in PyTorch handles the 4-dimensional\n",
    "input tensor so that the matrix multiplication is carried out between the 2 last dimensions\n",
    "(num_tokens, head_dim) and then repeated for the individual heads.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "For instance, the above becomes a more compact way to compute the matrix\n",
    "multiplication for each head separately:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "Second head:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n"
     ]
    }
   ],
   "source": [
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\n\", second_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The results are exactly the same results that we obtained when using the batched matrix\n",
    "multiplication print(a @ a.transpose(2, 3)) earlier:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Continuing with MultiHeadAttention, after computing the attention weights and context\n",
    "vectors, the context vectors from all heads are transposed back to the shape (b,\n",
    "num_tokens, num_heads, head_dim). \n",
    "\n",
    "These vectors are then reshaped (flattened) into the\n",
    "shape (b, num_tokens, d_out), effectively combining the outputs from all heads\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Additionally, we added a so-called output projection layer (self.out_proj) to\n",
    "MultiHeadAttention after combining the heads, which is not present in the\n",
    "CausalAttention class. \n",
    "\n",
    "This output projection layer is not strictly necessary, but it is commonly used in many LLM\n",
    "architectures, which is why we added it here for completeness.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Even though the MultiHeadAttention class looks more complicated than the\n",
    "MultiHeadAttentionWrapper due to the additional reshaping and transposition of tensors,\n",
    "it is more efficient. \n",
    "\n",
    "The reason is that we only need one matrix multiplication to compute\n",
    "the keys, for instance, keys = self.W_key(x) (the same is true for the queries and\n",
    "values). \n",
    "                                              \n",
    "\n",
    "In the MultiHeadAttentionWrapper, we needed to repeat this matrix multiplication,\n",
    "which is computationally one of the most expensive steps, for each attention head.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The MultiHeadAttention class can be used similar to the SelfAttention and\n",
    "CausalAttention classes we implemented earlier:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see based on the results, the output dimension is directly controlled by the\n",
    "d_out argument:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In this section, we implemented the MultiHeadAttention class that we will use in the\n",
    "upcoming sections when implementing and training the LLM itself. \n",
    "\n",
    "\n",
    "Note that while the code is fully functional, we used relatively small embedding sizes and numbers of attention\n",
    "heads to keep the outputs readable.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "For comparison, the smallest GPT-2 model (117 million parameters) has 12 attention\n",
    "heads and a context vector embedding size of 768. \n",
    "\n",
    "The largest GPT-2 model (1.5 billion\n",
    "parameters) has 25 attention heads and a context vector embedding size of 1600.\n",
    "\n",
    "Note\n",
    "that the embedding sizes of the token inputs and context embeddings are the same in GPT\n",
    "models (d_in = d_out).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
